{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HAN-DC.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.4"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "FvXupiQ89R6o",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "523e5354-70b0-43b4-99be-833e90fc8129"
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "from bs4 import BeautifulSoup\n",
        "import os\n",
        "from keras.preprocessing.text import Tokenizer, text_to_word_sequence\n",
        "from keras.utils import plot_model\n",
        "from keras.utils.np_utils import to_categorical\n",
        "from keras.layers import Dense, Input\n",
        "from keras.layers import Embedding, GRU, Bidirectional, TimeDistributed, Lambda\n",
        "from keras.models import Model\n",
        "from keras import initializers\n",
        "from keras import backend as K\n",
        "from keras.engine.topology import Layer, InputSpec\n",
        "from keras import initializers\n",
        "from keras.layers import Embedding\n",
        "from keras.layers import Dense, Input, Flatten\n",
        "from keras.layers import Conv1D, MaxPooling1D, Embedding, Dropout, LSTM, GRU, Bidirectional, TimeDistributed\n",
        "from keras.models import Model\n",
        "import pandas as pd\n",
        "import pickle as cPickle\n",
        "from collections import defaultdict\n",
        "from keras.preprocessing.text import Tokenizer, text_to_word_sequence\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from nltk import tokenize\n",
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xgMSLHlxyyqi"
      },
      "source": [
        "MAX_SENTENCE_LENGTH = 100\n",
        "MAX_SENTS = 15\n",
        "NB_WORDS = 20000\n",
        "EMBEDDING_DIMENSION = 100\n",
        "VAL_SPLIT = 0.2"
      ],
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2rt-KFJPyyqi"
      },
      "source": [
        "def string_cleaner(string):\n",
        "    string = re.sub(r\"\\\\\", \"\", string)\n",
        "    string = re.sub(r\"\\'\", \"\", string)\n",
        "    string = re.sub(r\"\\\"\", \"\", string)\n",
        "    return string.strip().lower()"
      ],
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3AxpgVb3yyqj"
      },
      "source": [
        "data_train = pd.read_csv('labeledTrainData.tsv', sep='\\t')"
      ],
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LAHgpc626OVA",
        "outputId": "256b6561-52be-4680-c9c1-ae2d7748ca61"
      },
      "source": [
        "print (data_train.shape)"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(25000, 3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2SiPIJHYyyqj"
      },
      "source": [
        "review = []\n",
        "w_label = []\n",
        "texts_tt = []\n",
        "for idx in range(data_train.review.shape[0]):\n",
        "    text_tt = BeautifulSoup(data_train.review[idx])\n",
        "    text_tt = string_cleaner(text_tt.get_text().encode('ascii', 'ignore').decode('ascii'))\n",
        "    texts_tt.append(text_tt)\n",
        "    sents = tokenize.sent_tokenize(text_tt)\n",
        "    review.append(sents)\n",
        "    w_label.append(data_train.sentiment[idx])"
      ],
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5K9DlsN_yyqk",
        "outputId": "495e797b-268b-403b-fc4e-181aa97416df"
      },
      "source": [
        "tokenizer = Tokenizer(nb_words=NB_WORDS)\n",
        "tokenizer.fit_on_texts(texts_tt)"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras_preprocessing/text.py:180: UserWarning: The `nb_words` argument in `Tokenizer` has been renamed `num_words`.\n",
            "  warnings.warn('The `nb_words` argument in `Tokenizer` '\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZwDGPX1pyyqk"
      },
      "source": [
        "w_data = np.zeros((len(texts_tt), MAX_SENTENCE_LENGTH), dtype='int32')\n",
        "for i, con in enumerate(texts_tt):\n",
        "    wordTokens = text_to_word_sequence(con)\n",
        "    j = 0\n",
        "    for _, W in enumerate(wordTokens):\n",
        "        if j < MAX_SENTENCE_LENGTH and tokenizer.word_index[W] < NB_WORDS:\n",
        "            w_data[i, j] = tokenizer.word_index[W]\n",
        "            j = j + 1"
      ],
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fpbvs_bPyyqk"
      },
      "source": [
        "w_index = tokenizer.word_index"
      ],
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FFDIjTkH8Gf4",
        "outputId": "5d8edd21-ab3c-4aa7-9c90-c6eb9a8fc9e3"
      },
      "source": [
        "print('There are a total of %s unique tokens.' % len(w_index))"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There are a total of 80566 unique tokens.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sHq7ib00yyqk"
      },
      "source": [
        "w_label = to_categorical(np.asarray(w_label))"
      ],
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cGr5zr-X8H98",
        "outputId": "8de633f6-e7c5-4f88-8feb-085c17330483"
      },
      "source": [
        "print('The shape of the data tensor is:', w_data.shape)\n",
        "print('The shape of the label tensor is:', w_label.shape)"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The shape of the data tensor is: (25000, 100)\n",
            "The shape of the label tensor is: (25000, 2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0WdkUv_Uyyql"
      },
      "source": [
        "inx = np.arange(w_data.shape[0])\n",
        "np.random.shuffle(inx)\n",
        "w_data = w_data[inx]\n",
        "w_label = w_label[inx]\n",
        "validation_samples = int(VAL_SPLIT * w_data.shape[0])"
      ],
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L1vbM6lxyyql"
      },
      "source": [
        "x_train = w_data[:-validation_samples]\n",
        "y_train = w_label[:-validation_samples]\n",
        "x_val = w_data[-validation_samples:]\n",
        "y_val = w_label[-validation_samples:]"
      ],
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P3G4zKsVyyql",
        "outputId": "d2946e03-c1fc-45cc-fe82-940bce290e4e"
      },
      "source": [
        "print('Positive & Negative reviews in train & val set respectively are: ')\n",
        "print (y_train.sum(axis=0))\n",
        "print (y_val.sum(axis=0))"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Positive & Negative reviews in train & val set respectively are: \n",
            "[10025.  9975.]\n",
            "[2475. 2525.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mQd5rzwXyyql",
        "outputId": "15c36155-394a-409d-db48-e39475ae04f4"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "GLOVE_DIR = \".\"\n",
        "embeddings_INDEX = {}\n",
        "f = open(os.path.join(GLOVE_DIR, '/content/gdrive/My Drive/glove/glove.6B.100d.txt'))\n",
        "for line in f:\n",
        "    values = line.split()\n",
        "    W = values[0]\n",
        "    coeficients = np.asarray(values[1:], dtype='float32')\n",
        "    embeddings_INDEX[W] = coeficients\n",
        "f.close()"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jCsEdr9Ryyql",
        "outputId": "bbabbe4a-dd78-47c7-ff4c-f857c64ba141"
      },
      "source": [
        "print('There are a total number of %s word vectors.' % len(embeddings_INDEX))"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There are a total number of 400000 word vectors.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HlkBmbY9yyql"
      },
      "source": [
        "embedding_MAT = np.random.random((len(w_index) + 1, EMBEDDING_DIMENSION))\n",
        "for W, i in w_index.items():\n",
        "    embedding_VEC = embeddings_INDEX.get(W)\n",
        "    if embedding_VEC is not None:\n",
        "        embedding_MAT[i] = embedding_VEC"
      ],
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-KWq9ptayyqm"
      },
      "source": [
        "embedding_layer = Embedding(len(w_index) + 1,\n",
        "                            EMBEDDING_DIMENSION,\n",
        "                            weights=[embedding_MAT],\n",
        "                            input_length=MAX_SENTENCE_LENGTH,\n",
        "                            trainable=True,\n",
        "                            mask_zero=True)"
      ],
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MBA9dKotyyqm"
      },
      "source": [
        "class Attention_Layer(Layer):\n",
        "    def __init__(self, attention_dim, **kwargs):\n",
        "        self.init = initializers.get('normal')\n",
        "        self.supports_masking = True\n",
        "        self.attention_dim = attention_dim\n",
        "        super(Attention_Layer, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        assert len(input_shape) == 3\n",
        "        self._W = K.variable(self.init((input_shape[-1], self.attention_dim)))\n",
        "        self._b = K.variable(self.init((self.attention_dim,)))\n",
        "        self._u = K.variable(self.init((self.attention_dim, 1)))\n",
        "        self._trainable_weights = [self._W, self._b, self._u]\n",
        "        super(Attention_Layer, self).build(input_shape)\n",
        "\n",
        "    def compute_mask(self, inputs, mask=None):\n",
        "        return None\n",
        "\n",
        "    def call(self, x, mask=None):\n",
        "        _uit = K.tile(K.expand_dims(self._W, axis=0), (K.shape(x)[0], 1, 1))\n",
        "        _uit = tf.matmul(x, _uit)\n",
        "        _uit = K.tanh(K.bias_add(_uit, self._b))\n",
        "        _ait = K.dot(_uit, self._u)\n",
        "        _ait = K.squeeze(_ait, -1)\n",
        "        _ait = K.exp(_ait)\n",
        "\n",
        "        if mask is not None:\n",
        "            _ait *= K.cast(mask, K.floatx())\n",
        "        \n",
        "        _ait /= K.cast(K.sum(_ait, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
        "        _ait = K.expand_dims(_ait)\n",
        "        weighted_input = x * _ait\n",
        "        output = K.sum(weighted_input, axis=1)\n",
        "        return output\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return (input_shape[0], input_shape[2])\n",
        "\n",
        "    def get_config(self):\n",
        "        config = {'attention_dim': self.attention_dim}\n",
        "        base_config = super(Attention_Layer, self).get_config()\n",
        "        return dict(list(base_config.items()) + list(config.items()))\n"
      ],
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nps-Rk-4HWZJ"
      },
      "source": [
        "sent_input = Input(shape=(MAX_SENTENCE_LENGTH,), dtype='int32')\n",
        "embedded_seq = embedding_layer(sent_input)\n",
        "L_lstm = Bidirectional(GRU(100, return_sequences=True))(embedded_seq)\n",
        "L_dense = TimeDistributed(Dense(200))(L_lstm)\n",
        "L_att = Attention_Layer(100)(L_dense)"
      ],
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eq3zADGbHc86"
      },
      "source": [
        "pred = Dense(2, activation='softmax')(L_att)"
      ],
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EeZtVqo-9_Rp"
      },
      "source": [
        "model = Model(sent_input, pred)"
      ],
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zYi6ncaBS2eY",
        "outputId": "a5900090-ed3a-4015-a471-90f44ee0b605"
      },
      "source": [
        "print(x_train.shape)"
      ],
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(20000, 100)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v8DAZxtES7Ow",
        "outputId": "1ef76d4f-caef-4bc5-a696-1587c7d1e793"
      },
      "source": [
        "print(y_train.shape)"
      ],
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(20000, 2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xIkWhCGPS-WX",
        "outputId": "df9a3661-8af6-42b1-a1bd-5dd040a1db70"
      },
      "source": [
        "print(x_val.shape)"
      ],
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(5000, 100)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UqmGdFIYTEvq",
        "outputId": "937955b3-8ee5-4aab-b3d8-643c5d441d01"
      },
      "source": [
        "print(y_val.shape)"
      ],
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(5000, 2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fhXVxMJkTYDu",
        "outputId": "90897cde-b4b9-40b3-bad6-94a1f6d05cf8"
      },
      "source": [
        "plot_model(model, to_file=\"Model.png\")\n",
        "\n",
        "model.summary()\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "print(\"********MODEL FITTING - HAN**************\")\n",
        "model.fit(x_train, y_train, validation_data=(x_val, y_val),epochs=10, batch_size=50,verbose=2)"
      ],
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_7 (InputLayer)         [(None, 100)]             0         \n",
            "_________________________________________________________________\n",
            "embedding_2 (Embedding)      (None, 100, 100)          8056700   \n",
            "_________________________________________________________________\n",
            "bidirectional_5 (Bidirection (None, 100, 200)          121200    \n",
            "_________________________________________________________________\n",
            "time_distributed_5 (TimeDist (None, 100, 200)          40200     \n",
            "_________________________________________________________________\n",
            "attention__layer_5 (Attentio (None, 200)               20200     \n",
            "_________________________________________________________________\n",
            "dense_9 (Dense)              (None, 2)                 402       \n",
            "=================================================================\n",
            "Total params: 8,238,702\n",
            "Trainable params: 8,238,702\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "********MODEL FITTING - HAN**************\n",
            "Epoch 1/10\n",
            "400/400 - 135s - loss: 0.4505 - accuracy: 0.7826 - val_loss: 0.3675 - val_accuracy: 0.8360\n",
            "Epoch 2/10\n",
            "400/400 - 127s - loss: 0.2953 - accuracy: 0.8763 - val_loss: 0.3648 - val_accuracy: 0.8396\n",
            "Epoch 3/10\n",
            "400/400 - 127s - loss: 0.1804 - accuracy: 0.9289 - val_loss: 0.4168 - val_accuracy: 0.8372\n",
            "Epoch 4/10\n",
            "400/400 - 126s - loss: 0.0797 - accuracy: 0.9722 - val_loss: 0.5967 - val_accuracy: 0.8358\n",
            "Epoch 5/10\n",
            "400/400 - 126s - loss: 0.0287 - accuracy: 0.9902 - val_loss: 0.9590 - val_accuracy: 0.8244\n",
            "Epoch 6/10\n",
            "400/400 - 127s - loss: 0.0140 - accuracy: 0.9954 - val_loss: 1.0498 - val_accuracy: 0.8312\n",
            "Epoch 7/10\n",
            "400/400 - 127s - loss: 0.0069 - accuracy: 0.9975 - val_loss: 1.3951 - val_accuracy: 0.8322\n",
            "Epoch 8/10\n",
            "400/400 - 127s - loss: 0.0084 - accuracy: 0.9974 - val_loss: 1.2728 - val_accuracy: 0.8286\n",
            "Epoch 9/10\n",
            "400/400 - 126s - loss: 0.0074 - accuracy: 0.9974 - val_loss: 1.2923 - val_accuracy: 0.8334\n",
            "Epoch 10/10\n",
            "400/400 - 127s - loss: 0.0066 - accuracy: 0.9980 - val_loss: 1.5802 - val_accuracy: 0.8286\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fc9e982a890>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 88
        }
      ]
    }
  ]
}